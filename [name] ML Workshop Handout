{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Linear Regression Workshop","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPLJ3k9/yt88qNJ+0SvxAn6"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"I3-yVVUnF_zn","colab_type":"text"},"source":["# Introduction to Machine Learning w/ Gradient Descent: Workshop #2\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","#####Pratical implementation with NumPy Matricies\n","#####A big thank you to Andrew Ng's Stanford Online Coursera course for workshop materials"]},{"cell_type":"code","metadata":{"id":"1qGrxF_WJgf0","colab_type":"code","outputId":"d8cdc697-9141-4db5-ce85-6f6bdcaa393d","executionInfo":{"status":"ok","timestamp":1582274616784,"user_tz":480,"elapsed":10704,"user":{"displayName":"Brian Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAZ6UBK6L-ViMxsn0jzslF7rTdo-tHvkQRGKBxszg=s64","userId":"16803751930876923121"}},"colab":{"base_uri":"https://localhost:8080/","height":161}},"source":["#grab dataset\n","pip install gdown"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.21.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.12.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.28.1)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2019.11.28)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Oe-me1cvJmbU","colab_type":"code","outputId":"ac7e4f47-5cf4-45f7-88ac-574630076e38","executionInfo":{"status":"ok","timestamp":1582274622780,"user_tz":480,"elapsed":14655,"user":{"displayName":"Brian Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAZ6UBK6L-ViMxsn0jzslF7rTdo-tHvkQRGKBxszg=s64","userId":"16803751930876923121"}},"colab":{"base_uri":"https://localhost:8080/","height":161}},"source":["#something super neat is that you can incorporate bash commands into your ipython notebook\n","!gdown --id 19e22YB3frVGktRiM4MUC5K4SPgdgtNi0 --output /content/data1.txt\n","!gdown --id 1UOncKnrj0xsiw8-9od5VpnaDu5xcQSC1 --output /content/data2.txt"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=19e22YB3frVGktRiM4MUC5K4SPgdgtNi0\n","To: /content/data1.txt\n","100% 1.36k/1.36k [00:00<00:00, 3.09MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1UOncKnrj0xsiw8-9od5VpnaDu5xcQSC1\n","To: /content/data2.txt\n","100% 657/657 [00:00<00:00, 548kB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R0THLu97JD1g","colab_type":"code","colab":{}},"source":["#@title Imports\n","#for managing directory paths & getting data\n","import os\n","\n","# Scientific and vector computation for Python, Matricies\n","import numpy as np\n","\n","# Plotting library\n","from matplotlib import pyplot\n","from mpl_toolkits.mplot3d import Axes3D  # needed to plot 3-D surfaces\n","\n","# tells matplotlib to embed plots within the notebook\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"16VS9ItcKB2s","colab_type":"code","colab":{}},"source":["#@title Introduction to matricies\n","\n","#matrices are multidimensional lists of numbers that can be operated in specific ways, and have unique properties\n","#The neat thing about them is that they allow us to operate on a set of numbers at the same time, replacing the need\n","#for a for loop\n","#Let's take a look at a few\n","\n","mat1 = np.random.randint(25, size = (5,5))\n","mat2 = np.random.randint(25, size = (5,5))\n","matID = np.eye(5)\n","print(\"mat1: \\n\", mat1)\n","print(\"mat2: \\n\", mat2)\n","\n","print(\"mat1 + mat2\\n\", mat1+mat2)\n","print(\"mat1 - mat2\\n\", mat1-mat2)\n","print(\"mat1 * mat2\\n\", mat1*mat2)\n","print(\"mat1 / mat2\\n\", mat1/mat2)\n","\n","print(\"np.dot(mat1, mat2)\\n\", np.dot(mat1, mat2))\n","\n","#there are special types of matrix and vector operations where only certain components are considered and acted on\n","#the dot product and cross product are examples. We will be uisng the dot product extensively \n","#we will not go into detail for now, but feel free to search it up\n","#you will learn more matrix stuff with Mr.Ark's senior c++ programming course\n","#matricies are everywhere. You will find a lot of them in post-secondary physics\n","\n","#there are many other properties of matricies, but we won't cover them for now. "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGziohmcMi77","colab_type":"code","colab":{}},"source":["#@title Let's take a look at the training data\n","#data is population of city vs profit for a product sold \n","\n","#load data\n","data = np.loadtxt(os.path.join('/content', 'data1.txt'), delimiter=',')\n","X, y = data[:, 0], data[:, 1]\n","m = y.size  # number of training examples"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TZ7oGjlrOigK","colab_type":"code","colab":{}},"source":["def plotData(X,y):\n","  fig = pyplot.figure()  # open a new figure\n","  pyplot.plot(X, y, 'ro', ms=10, mec='k')\n","  pyplot.ylabel('Profit in $10,000')\n","  pyplot.xlabel('Population of City in 10,000s')\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EsldMg9eOvaa","colab_type":"code","colab":{}},"source":["plotData(X,y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zEv2AkZ9Rta6","colab_type":"code","colab":{}},"source":["#add another dimension to the data (X must have dimensions (X, n+1), where X is the number of examples, and n the number of features. \n","#do the +1 \n","X = np.stack([np.ones(m), X], axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CfHFAITTO2DP","colab_type":"text"},"source":["### Cost Function\n","![Cost Function](http://s0.wp.com/latex.php?latex=J%28%5Ctheta%29+%3D+%5Cfrac%7B1%7D%7B2m%7D%5Csum%7B%28h_%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29+-+y%5E%7B%28i%29%7D%29%5E2%7D&bg=ffffff&fg=000&s=0)\n","#####sorry if it is so small :(\n","#### Compute the cost of a particular choice of theta. You should set J to the cost. Returns the cost (float)"]},{"cell_type":"code","metadata":{"id":"PNOSgIZ-OyKK","colab_type":"code","colab":{}},"source":["def costFunction(X, y, theta):\n","\n","  return cost"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sA7Ia7UBQVuS","colab_type":"code","colab":{}},"source":["#Let's take a look at the current cost (untrained theta set at [0,0] and [-5, 25] )\n","#theta is m and b in y = mx + b\n","print(\"Cost: \", costFunction(X, y, theta = np.array([0.0,0.0])), \"%\" )\n","print(\"Cost: \", costFunction(X, y, theta = np.array([-5,25])), \"%\" )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fycrueaz35bL","colab_type":"text"},"source":["#Gradient Descent\n","![alt text](https://miro.medium.com/max/450/1*8Omixzi4P2mnqdsPwIR1GQ.png)\n","####   Peform a single gradient step on the parameter vector theta. While debugging, it can be useful to print out the values of the cost function (computeCost) and gradient.\n","####Returns the new value of theta (float) and list of costs (float)"]},{"cell_type":"code","metadata":{"id":"SmZPtdNdSHtg","colab_type":"code","colab":{}},"source":["def gradientDescent(X, y, theta, alpha, iterations):\n","  return newTheta, costHistory"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UeyFs9jLSEQQ","colab_type":"code","colab":{}},"source":["#use some arbritary value for initial weights\n","theta = np.zeros(2) #[0,0]\n","iterations = 2000\n","alpha = 0.001 #learning rate"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YuPcZd_o5Oy9","colab_type":"code","colab":{}},"source":["#train!\n","theta, cost_history = gradientDescent(X, y, theta, alpha, iterations)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ys14mUP95E9d","colab_type":"code","outputId":"1a4866ec-4a80-4343-92c9-05cf19717ba6","executionInfo":{"status":"ok","timestamp":1582274643229,"user_tz":480,"elapsed":901,"user":{"displayName":"Brian Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAZ6UBK6L-ViMxsn0jzslF7rTdo-tHvkQRGKBxszg=s64","userId":"16803751930876923121"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(\"new weights: \", theta)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["new weights:  [-1.12369018  0.91454707]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_CytIXA76K89","colab_type":"code","colab":{}},"source":["pyplot.plot(np.arange(len(cost_history)), cost_history)\n","pyplot.xlabel(\"num iterations\")\n","pyplot.ylabel(\"cost\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANIHXW4d5aIs","colab_type":"code","colab":{}},"source":["# plot the linear fit\n","plotData(X[:, 1], y)\n","pyplot.plot(X[:, 1], np.dot(X, theta), '-')\n","pyplot.legend(['Training data', 'Linear regression']);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L9rOoOmU5zyU","colab_type":"code","colab":{}},"source":["def predict(i, weights):\n","  return prediction"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jer_YhsS_g0Z","colab_type":"code","colab":{}},"source":["print(\"for a population of 35,000,  the predicted profit to be: $\", predict([1, 3.5], theta), \"x10^4 \" )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HbZlgqsU6kqG","colab_type":"code","cellView":"form","colab":{}},"source":["#@title A visualization\n","# grid over which we will calculate J\n","theta0_vals = np.linspace(-10, 10, 100)\n","theta1_vals = np.linspace(-1, 4, 100)\n","\n","# initialize J_vals to a matrix of 0's\n","J_vals = np.zeros((theta0_vals.shape[0], theta1_vals.shape[0]))\n","\n","# Fill out J_vals\n","for i, theta0 in enumerate(theta0_vals):\n","    for j, theta1 in enumerate(theta1_vals):\n","        J_vals[i, j] = costFunction(X, y, [theta0, theta1])\n","        \n","# Because of the way meshgrids work in the surf command, we need to\n","# transpose J_vals before calling surf, or else the axes will be flipped\n","J_vals = J_vals.T\n","\n","# surface plot\n","fig = pyplot.figure(figsize=(12, 5))\n","ax = fig.add_subplot(121, projection='3d')\n","ax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap='viridis')\n","pyplot.xlabel('theta0')\n","pyplot.ylabel('theta1')\n","pyplot.title('Surface')\n","\n","# contour plot\n","# Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100\n","ax = pyplot.subplot(122)\n","pyplot.contour(theta0_vals, theta1_vals, J_vals, linewidths=2, cmap='viridis', levels=np.logspace(-2, 3, 20))\n","pyplot.xlabel('theta0')\n","pyplot.ylabel('theta1')\n","pyplot.plot(theta[0], theta[1], 'ro', ms=10, lw=2)\n","pyplot.title('Contour, showing minimum')\n","pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7jWnTixi7nk9","colab_type":"code","colab":{}},"source":["#Let's try it with more variable!\n","#load data\n","# Load data\n","data = np.loadtxt('data2.txt', delimiter=',')\n","X = data[:, :2]\n","y = data[:, 2]\n","m = y.size\n","\n","for i in range(10):\n","  print(\"X[:,0]: (living room size) \", X[i,0], \" X[:,1]: (num bedrooms) \", X[i, 1], \" y: \",  y[i], )\n","\n","#notice how there is a significant difference in size : >10^3 factor\n","#feature normalize data\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2DTA9TcMj7Pi","colab_type":"text"},"source":["#Feature normalization\n","##reduce range of X data so that things compute faster\n","####First, for each feature dimension, compute the mean of the feature and subtract it from the dataset, storing the mean value in mu. Next, compute the  standard deviation of each feature and divide each feature by it's standard deviation, storing the standard deviation in sigma. \n","    \n","####Note that X is a matrix where each column is a feature and each row is an example. You needto perform the normalization separately for each feature. \n","\n","####Given matrix X of dimensions (m x n), return normalized function Xnorm of dimensions (m x n)\n","    \n"," "]},{"cell_type":"code","metadata":{"id":"oSZCX5Es79IF","colab_type":"code","colab":{}},"source":["def fNormalize(X):\n","\n","  return mean, standardDeviation, newX\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WqsGW0s29D8e","colab_type":"code","colab":{}},"source":["mean, std, X = fNormalize(X)\n","print(\"mean: \", mean)\n","print(\"standard deviation: \", std)\n","print(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PNDDnCS49WLJ","colab_type":"code","colab":{}},"source":["def costFunction2(X, y, theta):\n","  #with matricies, will this be any different than before?\n","  return cost"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hap3A4We-ZW3","colab_type":"code","colab":{}},"source":["def gradientDescent2(X, y, theta, alpha, iterations):\n","  #notice how this is the same as before: matrices do all the work for us\n","\n","  return theta, costHistory"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbT-i6Mc-0Sc","colab_type":"code","colab":{}},"source":["#some parameters\n","alpha = 0.01\n","iterations = 10000\n","theta = np.zeros(3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s_QN10Gt_E-d","colab_type":"code","colab":{}},"source":["theta, costHistory = gradientDescent2(X, y, theta, alpha, iterations)\n","print(\"weights: \", theta)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJ1gYAbfA3hT","colab_type":"code","colab":{}},"source":["pyplot.plot(np.arange(len(cost_history)), cost_history)\n","pyplot.xlabel(\"num iterations\")\n","pyplot.ylabel(\"cost\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U5tdjzgDA4pS","colab_type":"code","colab":{}},"source":["house = [1, 5000, 4]\n","house[1:3] = (house[1:3]-mean)/std #have to normalize this too\n","print('prediction: $', predict(house, theta)) #predicted cost for a house with a living room size of 5000 and 4 bedrooms"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XO0gJnhuCIuQ","colab_type":"text"},"source":["#bonus: normal equation\n","![alt text](https://render.githubusercontent.com/render/math?math=%5Ctheta%20%3D%20%5Cleft%28%20X%5ET%20X%5Cright%29%5E%7B-1%7D%20X%5ET%5Cvec%7By%7D&mode=display)\n","##closed form solution: no need for algorithms\n","#####Why don't we just use this for everything? Gets very, very complex and slow for more complex data and more complex types of best fit curves"]},{"cell_type":"code","metadata":{"id":"7Q5uYJk0DsRi","colab_type":"code","colab":{}},"source":["def normalEquation(X, y):\n","  return theta"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rgcMpJiEact","colab_type":"code","colab":{}},"source":["#load raw data again\n","data = np.loadtxt('data2.txt', delimiter=',')\n","X = data[:, :2]\n","y = data[:, 2]\n","m = y.size\n","X = np.concatenate([np.ones((m, 1)), X], axis=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rW3EIrnWENfd","colab_type":"code","colab":{}},"source":["normalEquationWeights = normalEquation(X, y)\n","print(normalEquationWeights)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k6OG_s-xE6ry","colab_type":"code","colab":{}},"source":["predict(house, normalEquationWeights)\n","#why is this different from gradient descent-predicted?"],"execution_count":0,"outputs":[]}]}